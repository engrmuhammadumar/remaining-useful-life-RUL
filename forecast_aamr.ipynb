{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a27e00a",
   "metadata": {},
   "source": [
    "\n",
    "# AAMR Forecasting (ARIMA/SARIMAX) — Step‑by‑Step Notebook\n",
    "\n",
    "This notebook forecasts **Age-Adjusted Mortality Rates (AAMR)** out to a target year using **statsmodels SARIMAX**.\n",
    "It includes:\n",
    "- Robust year parsing (handles dates and messy strings)\n",
    "- Annual PeriodIndex (no \"unsupported index\" warnings)\n",
    "- Option to handle 2020–2021 as **none / dummy / drop**\n",
    "- Simple **auto-order** search using AICc\n",
    "- Rolling backtest (**MAE/RMSE/MAPE**)\n",
    "- Exports tidy CSVs, plots, and a manifest\n",
    "\n",
    "> **How to use**: Edit the **Configuration** cell below (paths + options), then **Run All**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b25aa",
   "metadata": {},
   "source": [
    "## 0. (Optional) Install/Update Libraries\n",
    "Run this if your environment is missing any dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, uncomment and run:\n",
    "# %pip install --upgrade pip\n",
    "# %pip install pandas matplotlib statsmodels openpyxl xlrd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484cd949",
   "metadata": {},
   "source": [
    "## 1. Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be6f2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, glob, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# Use non-interactive backend when running in batch; VS Code/Jupyter will still show plots inline\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f81a16",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "374076b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === EDIT THESE ===\n",
    "DATA_DIR = r\"E:\\Collaboration Work\\With Farooq\\medical\\data\"  # <- folder with your Excel files\n",
    "FILE_GLOB = \"*.xlsx\"            # e.g., \"*.xlsx\" or \"AAMR_*.xlsx\"\n",
    "OUTPUT_DIR = \"forecast_output\"  # outputs go here (created if missing)\n",
    "\n",
    "END_YEAR = 2035                 # forecast through this year\n",
    "MIN_TRAIN = 12                  # minimum training length in years\n",
    "PANDEMIC_MODE = \"dummy\"         # \"none\" | \"dummy\" | \"drop\"\n",
    "\n",
    "# If the auto-target guess ever fails, set one of these:\n",
    "TARGET_COL_OVERRIDE = None      # e.g., \"Age Adjusted Rate\"\n",
    "TARGET_REGEX_OVERRIDE = None    # e.g., r\"(?i)age\\\\s*adj.*rate\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76646a2",
   "metadata": {},
   "source": [
    "## 3. Helper Functions — Parsing, Cleaning, Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb30c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AAMR_GUESS_PATTERNS = [\n",
    "    r\"\\bAAMR\\b\",\n",
    "    r\"age[-\\s_]*adj(?:usted)?(?:[-\\s_]*mortality)?[-\\s_]*rate\",\n",
    "    r\"age[-\\s_]*adjusted.*rate\",\n",
    "    r\"\\bage[-\\s_]*adj\\b\",\n",
    "    r\"\\bage[-\\s_]*standardi[sz]ed.*rate\",\n",
    "]\n",
    "YEAR_ALIAS = [r\"^year$\", r\"^yr$\", r\"^calendar\\s*year$\"]\n",
    "\n",
    "def normalize_col(c): \n",
    "    import re\n",
    "    return re.sub(r\"\\s+\", \" \", str(c)).strip()\n",
    "\n",
    "def find_year_col(cols):\n",
    "    import re\n",
    "    low = [str(c).lower().strip() for c in cols]\n",
    "    for pat in YEAR_ALIAS:\n",
    "        for i,c in enumerate(low):\n",
    "            if re.search(pat, c): \n",
    "                return cols[i]\n",
    "    for i,c in enumerate(low):\n",
    "        if \"year\" in c: \n",
    "            return cols[i]\n",
    "    return None\n",
    "\n",
    "def parse_year_series(s, lo=1900, hi=2100):\n",
    "    \"\"\"Extract 4-digit year from messy strings/dates, coerce invalid to NaN, bound to [lo,hi].\"\"\"\n",
    "    if pd.api.types.is_datetime64_any_dtype(s):\n",
    "        years = s.dt.year.astype(\"Float64\")\n",
    "    else:\n",
    "        ss = s.astype(str)\n",
    "        years = ss.str.extract(r\"(\\d{4})\", expand=False)\n",
    "        years = pd.to_numeric(years, errors=\"coerce\")\n",
    "    years = years.astype(\"Float64\")\n",
    "    years = years.where((years >= lo) & (years <= hi))\n",
    "    return years\n",
    "\n",
    "def find_target_col(df, explicit_name=None, target_regex=None):\n",
    "    import re\n",
    "    cols = list(df.columns)\n",
    "    if explicit_name and explicit_name in cols:\n",
    "        return explicit_name\n",
    "    if target_regex:\n",
    "        for c in cols:\n",
    "            if re.search(target_regex, str(c), flags=re.I):\n",
    "                return c\n",
    "    scores=[]\n",
    "    for c in cols:\n",
    "        s=0; name=str(c)\n",
    "        for pat in AAMR_GUESS_PATTERNS:\n",
    "            if re.search(pat, name, flags=re.I): s+=2\n",
    "        if re.search(r\"\\brate\\b\", name, flags=re.I): s+=1\n",
    "        if re.search(r\"crude\", name, flags=re.I): s-=3\n",
    "        if not pd.api.types.is_numeric_dtype(df[c]): s-=10\n",
    "        scores.append((s,c))\n",
    "    scores.sort(reverse=True)\n",
    "    best = scores[0][1] if scores else None\n",
    "    if best is not None and pd.api.types.is_numeric_dtype(df[best]): \n",
    "        return best\n",
    "    return None\n",
    "\n",
    "def clean_numeric(s):\n",
    "    if pd.api.types.is_numeric_dtype(s): \n",
    "        out = pd.to_numeric(s, errors=\"coerce\")\n",
    "    else:\n",
    "        out = (s.astype(str)\n",
    "                 .str.replace(\",\",\"\",regex=False)\n",
    "                 .str.replace(\"%\",\"\",regex=False)\n",
    "                 .str.extract(r\"([-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?)\")[0])\n",
    "        out = pd.to_numeric(out, errors=\"coerce\")\n",
    "    return out.astype(\"Float64\")\n",
    "\n",
    "def ensure_dir(p): \n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sanitize_filename(s): \n",
    "    import re\n",
    "    return re.sub(r\"[\\\\/:*?\\\"<>|]+\",\"_\",s)[:180]\n",
    "\n",
    "def as_annual_series(y_values, years):\n",
    "    idx = pd.PeriodIndex(np.asarray(years, dtype=int), freq='A-DEC')\n",
    "    return pd.Series(np.asarray(y_values, dtype=float), index=idx)\n",
    "\n",
    "def align_exog_to_years(exog_df, years):\n",
    "    if exog_df is None:\n",
    "        return None\n",
    "    idx = pd.PeriodIndex(np.asarray(years, dtype=int), freq='A-DEC')\n",
    "    X = exog_df.copy()\n",
    "    X.index = pd.PeriodIndex(np.asarray(X.index, dtype=int), freq='A-DEC')\n",
    "    return X.reindex(idx)\n",
    "\n",
    "def make_intervention_exog(years, mode):\n",
    "    if mode != \"dummy\": \n",
    "        return None\n",
    "    y = np.array(years, dtype=int)\n",
    "    X = pd.DataFrame({\n",
    "        \"d2020\": (y == 2020).astype(int),\n",
    "        \"d2021\": (y == 2021).astype(int)\n",
    "    }, index=years)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba91a6b",
   "metadata": {},
   "source": [
    "## 4. Modeling Helpers — Stationarity, AICc Grid, Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1eabb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def kpss_adf_diff_order(y, max_d=2):\n",
    "    d=0; yv=pd.Series(y)\n",
    "    while d<max_d and len(yv)>=3:\n",
    "        adf_p = adfuller(yv, autolag=\"AIC\")[1] if len(yv)>=5 else 1.0\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                kpss_p = kpss(yv, nlags=\"auto\")[1] if len(yv)>=8 else 0.0\n",
    "        except Exception:\n",
    "            kpss_p = 0.0\n",
    "        if adf_p<0.05 and kpss_p>0.05: break\n",
    "        yv = yv.diff().dropna(); d+=1\n",
    "    return d\n",
    "\n",
    "def aicc(llf, k, n):\n",
    "    return -2*llf + 2*k*n/(n-k-1) if (n-k-1)>0 else np.inf\n",
    "\n",
    "def fit_sarimax_aicc(y_series, exog, d, p_range=range(0,6), q_range=range(0,6)):\n",
    "    best=None; n=len(y_series)\n",
    "    for p in p_range:\n",
    "        for q in q_range:\n",
    "            if p==0 and q==0 and d==0: \n",
    "                continue\n",
    "            try:\n",
    "                m=SARIMAX(y_series, order=(p,d,q), exog=exog,\n",
    "                          trend=\"c\", enforce_stationarity=False,\n",
    "                          enforce_invertibility=False)\n",
    "                res=None\n",
    "                for method in (\"lbfgs\", \"powell\", \"nm\"):\n",
    "                    try:\n",
    "                        res=m.fit(disp=False, maxiter=500, method=method, concentrate_scale=True)\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        res=None\n",
    "                if res is None: \n",
    "                    continue\n",
    "                k=res.params.size; score=aicc(res.llf, k, n)\n",
    "                if (best is None) or (score<best[\"aicc\"]):\n",
    "                    best={\"res\":res,\"aicc\":score,\"order\":(p,d,q)}\n",
    "            except Exception:\n",
    "                continue\n",
    "    return best[\"res\"] if best else None\n",
    "\n",
    "def ljungbox_pvalue(residuals, lags=10):\n",
    "    try:\n",
    "        lb=acorr_ljungbox(residuals, lags=[lags], return_df=True)\n",
    "        return float(lb[\"lb_pvalue\"].iloc[0])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def predict_pi(res, n, exog_future=None, alpha=0.2):\n",
    "    fc = res.get_forecast(steps=n, exog=exog_future)\n",
    "    mean = fc.predicted_mean.values\n",
    "    conf = fc.conf_int(alpha=alpha).values\n",
    "    return mean, conf[:,0], conf[:,1]\n",
    "\n",
    "def plot_history_forecast(series_name, years, y, years_fc, yhat, lo80, hi80, lo95, hi95, outpath):\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    plt.plot(years, y, label=\"historical\")\n",
    "    if len(years_fc):\n",
    "        plt.plot(years_fc, yhat, label=\"forecast\")\n",
    "        plt.fill_between(years_fc, lo80, hi80, alpha=0.3, label=\"80% PI\")\n",
    "        plt.fill_between(years_fc, lo95, hi95, alpha=0.2, label=\"95% PI\")\n",
    "    plt.title(series_name); plt.xlabel(\"Year\"); plt.ylabel(\"AAMR\"); plt.legend(); plt.tight_layout()\n",
    "    fig.savefig(outpath, dpi=160); plt.close(fig)\n",
    "\n",
    "def rolling_backtest(y, years, exog, min_train, h=1):\n",
    "    preds=[]; actuals=[]; pred_years=[]\n",
    "    for i in range(min_train, len(y)):\n",
    "        y_train = as_annual_series(y[:i], years[:i])\n",
    "        X_train = align_exog_to_years(exog.iloc[:i] if exog is not None else None, years[:i])\n",
    "        X_test  = align_exog_to_years(exog.iloc[i:i+h] if exog is not None else None, years[i:i+h])\n",
    "        d=kpss_adf_diff_order(y_train.values)\n",
    "        res=fit_sarimax_aicc(y_train, X_train, d)\n",
    "        if res is None: continue\n",
    "        fc=res.get_forecast(steps=h, exog=X_test)\n",
    "        preds.append(float(fc.predicted_mean[-1]))\n",
    "        actuals.append(float(y[i])); pred_years.append(int(years[i]))\n",
    "    preds=np.array(preds); actuals=np.array(actuals); pred_years=np.array(pred_years)\n",
    "    mae=float(np.mean(np.abs(actuals-preds))) if len(preds) else np.nan\n",
    "    rmse=float(np.sqrt(np.mean((actuals-preds)**2))) if len(preds) else np.nan\n",
    "    mape=float(np.mean(np.abs((actuals-preds)/np.maximum(actuals,1e-12)))*100) if len(preds) else np.nan\n",
    "    return pred_years, preds, actuals, {\"MAE\":mae,\"RMSE\":rmse,\"MAPE\":mape}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38494527",
   "metadata": {},
   "source": [
    "## 5. Core Pipeline — Read, Process, Forecast, Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "012aa817",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 94) (2666599403.py, line 94)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"csv\":os.path.relpath(out_csv, out_dir).replace(\"\\\",\"/\"),\u001b[39m\n                                                          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 94)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_one_dataframe(df0, meta_name, args, global_outputs):\n",
    "    df=df0.copy(); df.columns=[normalize_col(c) for c in df.columns]\n",
    "\n",
    "    # robust YEAR\n",
    "    year_col=find_year_col(df.columns)\n",
    "    if year_col is None:\n",
    "        print(f\"[skip] {meta_name}: no Year col.\"); return\n",
    "    yrs = parse_year_series(df[year_col])\n",
    "    df = df.assign(__YEAR__=yrs).dropna(subset=[\"__YEAR__\"])\n",
    "    df[\"__YEAR__\"] = df[\"__YEAR__\"].astype(int)\n",
    "\n",
    "    # target\n",
    "    target_col=find_target_col(df, explicit_name=args.get(\"target_col\"), target_regex=args.get(\"target_regex\"))\n",
    "    if target_col is None:\n",
    "        print(f\"[skip] {meta_name}: no AAMR col.\"); return\n",
    "    df[target_col]=clean_numeric(df[target_col])\n",
    "    df=df.dropna(subset=[target_col])\n",
    "    df=df[np.isfinite(df[target_col])].sort_values(\"__YEAR__\")\n",
    "\n",
    "    # training mask for pandemic\n",
    "    if args[\"pandemic_mode\"]==\"drop\":\n",
    "        train_mask=~df[\"__YEAR__\"].isin([2020,2021])\n",
    "    else:\n",
    "        train_mask=np.ones(len(df), dtype=bool)\n",
    "\n",
    "    fit_and_forecast_series(df, \"__YEAR__\", target_col, meta_name, \"ALL\", train_mask, args, global_outputs)\n",
    "\n",
    "def fit_and_forecast_series(sub, year_col, target_col, file_sheet_name, series_key, train_mask, args, global_outputs):\n",
    "    out_dir=args[\"output_dir\"]\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    series_name=f\"{file_sheet_name} :: {series_key}\"\n",
    "    series_id=sanitize_filename(series_name)\n",
    "\n",
    "    y_all=sub[target_col].astype(float).values\n",
    "    years_all=sub[year_col].astype(int).values\n",
    "\n",
    "    idx=np.where(train_mask)[0]\n",
    "    y,years=y_all[idx],years_all[idx]\n",
    "    if len(y)<max(args[\"min_train\"],6):\n",
    "        print(f\"[skip] {series_name}: too few points ({len(y)})\"); return\n",
    "\n",
    "    # backtest\n",
    "    X_train0=make_intervention_exog(years, args[\"pandemic_mode\"])\n",
    "    bt_years,_,_,bt_metrics=rolling_backtest(y, years, X_train0, min_train=args[\"min_train\"], h=1)\n",
    "\n",
    "    # final fit on training window\n",
    "    y_train_series = as_annual_series(y, years)\n",
    "    X_train = align_exog_to_years(X_train0, years)\n",
    "    d=kpss_adf_diff_order(y_train_series.values)\n",
    "    res=fit_sarimax_aicc(y_train_series, X_train, d)\n",
    "    if res is None:\n",
    "        print(f\"[warn] {series_name}: model fail\"); return\n",
    "\n",
    "    # forecast horizon\n",
    "    last_year=int(years_all.max())\n",
    "    horizon=max(0, int(args[\"end_year\"])-last_year)\n",
    "    years_fc=np.arange(last_year+1, last_year+1+horizon, dtype=int)\n",
    "\n",
    "    if horizon>0:\n",
    "        X_future=make_intervention_exog(years_fc, args[\"pandemic_mode\"])\n",
    "        X_future=align_exog_to_years(X_future, years_fc)\n",
    "        yhat80, lo80, hi80 = predict_pi(res, horizon, X_future, alpha=0.20)\n",
    "        yhat95, lo95, hi95 = predict_pi(res, horizon, X_future, alpha=0.05)\n",
    "    else:\n",
    "        years_fc=[]; yhat95=lo80=hi80=lo95=hi95=[]\n",
    "\n",
    "    # export rows\n",
    "    out_rows=[]\n",
    "    for yr,val in zip(years_all,y_all):\n",
    "        out_rows.append({\"series\":series_name,\"year\":int(yr),\"aamr\":float(val),\n",
    "                         \"is_forecast\":0,\"lo80\":np.nan,\"hi80\":np.nan,\n",
    "                         \"lo95\":np.nan,\"hi95\":np.nan})\n",
    "    for i in range(len(years_fc)):\n",
    "        out_rows.append({\"series\":series_name,\"year\":int(years_fc[i]),\n",
    "                         \"aamr\":float(yhat95[i]),\"is_forecast\":1,\n",
    "                         \"lo80\":float(lo80[i]),\"hi80\":float(hi80[i]),\n",
    "                         \"lo95\":float(lo95[i]),\"hi95\":float(hi95[i])})\n",
    "    out_df=pd.DataFrame(out_rows)\n",
    "\n",
    "    out_csv=os.path.join(out_dir,\"series_csv\",f\"{series_id}.csv\")\n",
    "    ensure_dir(os.path.dirname(out_csv)); out_df.to_csv(out_csv,index=False)\n",
    "\n",
    "    plot_path=os.path.join(out_dir,\"plots\",f\"{series_id}.png\")\n",
    "    ensure_dir(os.path.dirname(plot_path))\n",
    "    plot_history_forecast(series_name, years_all, y_all, years_fc, yhat95, lo80, hi80, lo95, hi95, plot_path)\n",
    "\n",
    "    # metrics + index\n",
    "    global_outputs[\"metrics\"].append({\"series\":series_name,\"file_sheet\":file_sheet_name,\n",
    "                                      \"target_col\":target_col,\"n_obs\":int(len(y_all)),\n",
    "                                      \"train_n\":int(len(y)),\n",
    "                                      \"last_year\":int(years_all.max()), **bt_metrics})\n",
    "    global_outputs[\"index\"].append({\"series\":series_name,\n",
    "                                    \"csv\":os.path.relpath(out_csv, out_dir).replace(\"\\\",\"/\"),\n",
    "                                    \"plot\":os.path.relpath(plot_path, out_dir).replace(\"\\\",\"/\")})\n",
    "\n",
    "def load_excel_as_sheets(path):\n",
    "    sheets=pd.read_excel(path, sheet_name=None, engine=\"openpyxl\")\n",
    "    return {f\"{os.path.basename(path)} :: {s}\":df for s,df in sheets.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a011aeb",
   "metadata": {},
   "source": [
    "## 6. Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02c8ae0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] Obesity+DM+HTN POD corrected.xlsx: cannot read (name 'load_excel_as_sheets' is not defined)\n",
      "[skip] Obesity+DM+HTN age groups final.xlsx: cannot read (name 'load_excel_as_sheets' is not defined)\n",
      "[skip] Obesity+DM+HTN states corrected.xlsx: cannot read (name 'load_excel_as_sheets' is not defined)\n",
      "[skip] Obesity+DM+HTN urbanization final.xlsx: cannot read (name 'load_excel_as_sheets' is not defined)\n",
      "[skip] obesity+DM+HTN census region final.xlsx: cannot read (name 'load_excel_as_sheets' is not defined)\n",
      "[skip] obesity+DM+HTN gender final.xlsx: cannot read (name 'load_excel_as_sheets' is not defined)\n",
      "[skip] obesity+DM+HTN overall final.xlsx: cannot read (name 'load_excel_as_sheets' is not defined)\n",
      "[skip] obesity+DM+HTN race final.xlsx: cannot read (name 'load_excel_as_sheets' is not defined)\n",
      "[skip] ~$Obesity+DM+HTN age groups final.xlsx: cannot read (name 'load_excel_as_sheets' is not defined)\n",
      "Done.\n",
      "- Metrics: forecast_output\\metrics.csv\n",
      "- Series CSVs: forecast_output\\series_csv\n",
      "- Plots: forecast_output\\plots\n",
      "- Manifest: forecast_output\\manifest.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build args dict\n",
    "args = dict(\n",
    "    data_dir=DATA_DIR,\n",
    "    file_glob=FILE_GLOB,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    end_year=END_YEAR,\n",
    "    min_train=MIN_TRAIN,\n",
    "    pandemic_mode=PANDEMIC_MODE,\n",
    "    target_col=TARGET_COL_OVERRIDE,\n",
    "    target_regex=TARGET_REGEX_OVERRIDE,\n",
    ")\n",
    "\n",
    "# Gather files\n",
    "files = sorted(glob.glob(os.path.join(args[\"data_dir\"], args[\"file_glob\"])))\n",
    "if not files:\n",
    "    raise SystemExit(\"No files found. Check DATA_DIR and FILE_GLOB.\")\n",
    "\n",
    "global_outputs={\"metrics\":[], \"index\":[]}\n",
    "\n",
    "for fpath in files:\n",
    "    try:\n",
    "        sheets=load_excel_as_sheets(fpath)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {os.path.basename(fpath)}: cannot read ({e})\")\n",
    "        continue\n",
    "\n",
    "    for tag, df in sheets.items():\n",
    "        try:\n",
    "            process_one_dataframe(df, tag, args, global_outputs)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] processing failed for {tag}: {e}\")\n",
    "\n",
    "# Write outputs\n",
    "ensure_dir(args[\"output_dir\"])\n",
    "metrics_path = os.path.join(args[\"output_dir\"], \"metrics.csv\")\n",
    "pd.DataFrame(global_outputs[\"metrics\"]).to_csv(metrics_path, index=False)\n",
    "\n",
    "manifest_path = os.path.join(args[\"output_dir\"], \"manifest.json\")\n",
    "with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(global_outputs, f, indent=2)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(f\"- Metrics: {metrics_path}\")\n",
    "print(f\"- Series CSVs: {os.path.join(args['output_dir'],'series_csv')}\")\n",
    "print(f\"- Plots: {os.path.join(args['output_dir'],'plots')}\")\n",
    "print(f\"- Manifest: {manifest_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3225c2bc",
   "metadata": {},
   "source": [
    "## 7. Quick Checks — Metrics & First Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa372888",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, Image\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Show metrics head\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m m = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetrics.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m display(m.head(\u001b[32m10\u001b[39m))\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Try to display the first available plot\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Collaboration Work\\With Farooq\\medical\\arima_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Collaboration Work\\With Farooq\\medical\\arima_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Collaboration Work\\With Farooq\\medical\\arima_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Collaboration Work\\With Farooq\\medical\\arima_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Collaboration Work\\With Farooq\\medical\\arima_env\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:581\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Show metrics head\n",
    "m = pd.read_csv(os.path.join(OUTPUT_DIR, \"metrics.csv\"))\n",
    "display(m.head(10))\n",
    "\n",
    "# Try to display the first available plot\n",
    "try:\n",
    "    import os\n",
    "    plots_dir = os.path.join(OUTPUT_DIR, \"plots\")\n",
    "    plot_files = [os.path.join(plots_dir, x) for x in os.listdir(plots_dir) if x.endswith(\".png\")]\n",
    "    if plot_files:\n",
    "        display(Image(filename=plot_files[0]))\n",
    "except Exception as e:\n",
    "    print(\"Could not display plot:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7615d9",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Troubleshooting\n",
    "\n",
    "- **No files found**: Check `DATA_DIR` and `FILE_GLOB` in the config cell.\n",
    "- **Target column not found**: Set `TARGET_COL_OVERRIDE` to the exact AAMR column name, or `TARGET_REGEX_OVERRIDE` (e.g., `r\"(?i)age\\s*adj.*rate\"`).\n",
    "- **Convergence warnings**: Normal; the notebook tries multiple optimizers. If persistent, increase training years or try `PANDEMIC_MODE=\"drop\"`.\n",
    "- **Weird spikes around 2020–2021**: Try `PANDEMIC_MODE=\"dummy\"` (default) or `PANDEMIC_MODE=\"drop\"`.\n",
    "- **Accuracy**: Open `metrics.csv` and look at `MAPE/MAE/RMSE`. We can add ETS fallback if any series still fails.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arima_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

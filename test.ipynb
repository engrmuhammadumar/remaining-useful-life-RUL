{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91459172",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple, Dict, List\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cfg\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_wear_table\u001b[39m(cutter_dir: \u001b[38;5;28mstr\u001b[39m) -> Tuple[pd.DataFrame, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    127\u001b[39m     cands = [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m glob.glob(os.path.join(cutter_dir, \u001b[33m\"\u001b[39m\u001b[33m*.csv\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mwear\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os.path.basename(p).lower()]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# config.py\n",
    "# =========================\n",
    "\"\"\"\n",
    "Config for MonoSeq-RUL on PHM2010.\n",
    "Edit BASE to your dataset root. Everything else can stay as-is to start.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# 1) BASIC PATHS & SPLITS\n",
    "BASE = r\"E:\\\\Collaboration Work\\\\With Farooq\\\\phm dataset\\\\PHM Challange 2010 Milling\"\n",
    "TRAIN_CUTTERS = [\"c1\", \"c4\", \"c6\"]  # labeled\n",
    "TEST_CUTTERS  = [\"c2\", \"c3\", \"c5\"]  # unlabeled\n",
    "\n",
    "ROOT_OUT = Path(\"artifacts_monoseq\").resolve()\n",
    "FEAT_DIR = ROOT_OUT / \"features\"\n",
    "MODEL_DIR = ROOT_OUT / \"models\"\n",
    "PLOT_DIR  = ROOT_OUT / \"plots\"\n",
    "CSV_DIR   = ROOT_OUT / \"csv\"\n",
    "\n",
    "# 2) SIGNAL / WINDOW SETTINGS\n",
    "FS = 50_000\n",
    "WIN = 4096\n",
    "HOP = 2048\n",
    "MAX_WINDOWS = 96\n",
    "\n",
    "# 3) TRAINING SETTINGS\n",
    "SEED = 42\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-3\n",
    "EPOCHS = 35\n",
    "WEIGHT_DECAY = 1e-5\n",
    "GRAD_CLIP = 1.0\n",
    "VAL_RATIO = 0.15\n",
    "\n",
    "# Backbone\n",
    "HIDDEN = 256\n",
    "DROPOUT = 0.2\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "# Loss mixing\n",
    "LAMBDA_SMOOTH_DELTA = 0.1\n",
    "LAMBDA_PHASE = 0.0  # set 0.2 after we add phase labels\n",
    "\n",
    "# Conformal (reserved)\n",
    "CONF_ALPHA = 0.1\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    base: str = BASE\n",
    "    train_cutters: list[str] = None\n",
    "    test_cutters: list[str] = None\n",
    "    fs: int = FS\n",
    "    win: int = WIN\n",
    "    hop: int = HOP\n",
    "    max_windows: int = MAX_WINDOWS\n",
    "\n",
    "    seed: int = SEED\n",
    "    batch_size: int = BATCH_SIZE\n",
    "    lr: float = LR\n",
    "    epochs: int = EPOCHS\n",
    "    weight_decay: float = WEIGHT_DECAY\n",
    "    grad_clip: float = GRAD_CLIP\n",
    "    val_ratio: float = VAL_RATIO\n",
    "\n",
    "    hidden: int = HIDDEN\n",
    "    dropout: float = DROPOUT\n",
    "    num_layers: int = NUM_LAYERS\n",
    "\n",
    "    lambda_smooth_delta: float = LAMBDA_SMOOTH_DELTA\n",
    "    lambda_phase: float = LAMBDA_PHASE\n",
    "\n",
    "    conf_alpha: float = CONF_ALPHA\n",
    "\n",
    "    root_out: Path = ROOT_OUT\n",
    "    feat_dir: Path = FEAT_DIR\n",
    "    model_dir: Path = MODEL_DIR\n",
    "    plot_dir: Path = PLOT_DIR\n",
    "    csv_dir: Path = CSV_DIR\n",
    "\n",
    "    device: torch.device = DEVICE\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.train_cutters is None:\n",
    "            self.train_cutters = TRAIN_CUTTERS\n",
    "        if self.test_cutters is None:\n",
    "            self.test_cutters = TEST_CUTTERS\n",
    "\n",
    "def make_dirs(cfg: Config):\n",
    "    for p in [cfg.root_out, cfg.feat_dir, cfg.model_dir, cfg.plot_dir, cfg.csv_dir]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def show(cfg: Config):\n",
    "    print(\"\\n=== MonoSeq-RUL Config ===\")\n",
    "    for k, v in asdict(cfg).items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"Device:\", cfg.device)\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "cfg = Config()\n",
    "make_dirs(cfg)\n",
    "\n",
    "# =========================\n",
    "# data.py\n",
    "# =========================\n",
    "\"\"\"\n",
    "Data utilities for PHM2010 MonoSeq-RUL.\n",
    "- Reads wear CSVs for training cutters\n",
    "- Discovers cut CSV files for each cutter\n",
    "- Builds index lists (train/val/test)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, re, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "from config import cfg\n",
    "\n",
    "def read_wear_table(cutter_dir: str) -> Tuple[pd.DataFrame, float]:\n",
    "    cands = [p for p in glob.glob(os.path.join(cutter_dir, \"*.csv\")) if \"wear\" in os.path.basename(p).lower()]\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No wear csv in {cutter_dir}\")\n",
    "    wear_file = cands[0]\n",
    "\n",
    "    raw0 = pd.read_csv(wear_file, sep=None, engine=\"python\", nrows=5)\n",
    "    try:\n",
    "        v = pd.to_numeric(raw0.iloc[0, 0], errors=\"coerce\")\n",
    "        use_header = bool(pd.isna(v))\n",
    "    except Exception:\n",
    "        use_header = True\n",
    "\n",
    "    raw = (\n",
    "        pd.read_csv(wear_file, sep=None, engine=\"python\") if use_header\n",
    "        else pd.read_csv(wear_file, sep=None, engine=\"python\", header=None)\n",
    "    )\n",
    "    raw.columns = [str(c).strip().lower() for c in raw.columns]\n",
    "\n",
    "    def first_present(names):\n",
    "        for n in names:\n",
    "            if n in raw.columns:\n",
    "                return n\n",
    "        return None\n",
    "\n",
    "    cut_col = first_present([\"cut\", \"cut_number\", \"cut no\", \"cut_no\", \"c\", \"index\", \"id\", \"0\"])\n",
    "    f1_col  = first_present([\"flute_1\", \"flute1\", \"f1\", \"flute 1\", \"1\"])\n",
    "    f2_col  = first_present([\"flute_2\", \"flute2\", \"f2\", \"flute 2\", \"2\"])\n",
    "    f3_col  = first_present([\"flute_3\", \"flute3\", \"f3\", \"flute 3\", \"3\"])\n",
    "\n",
    "    if cut_col is None or f1_col is None or f2_col is None or f3_col is None:\n",
    "        tmp = raw.copy().dropna(axis=1, how=\"all\")\n",
    "        assert tmp.shape[1] >= 4, \"Wear file must have >=4 usable columns\"\n",
    "        tmp.columns = [f\"col_{i}\" for i in range(tmp.shape[1])]\n",
    "        cut_col, f1_col, f2_col, f3_col = \"col_0\", \"col_1\", \"col_2\", \"col_3\"\n",
    "        raw = tmp\n",
    "\n",
    "    cut_series = raw[cut_col].astype(str).str.extract(r\"(\\d+)\", expand=False)\n",
    "    cut_series = pd.to_numeric(cut_series, errors=\"coerce\")\n",
    "\n",
    "    f1 = pd.to_numeric(raw[f1_col], errors=\"coerce\")\n",
    "    f2 = pd.to_numeric(raw[f2_col], errors=\"coerce\")\n",
    "    f3 = pd.to_numeric(raw[f3_col], errors=\"coerce\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"cut_number\": cut_series,\n",
    "        \"flute_1\": f1, \"flute_2\": f2, \"flute_3\": f3\n",
    "    }).dropna()\n",
    "    df[\"cut_number\"] = df[\"cut_number\"].round().astype(int)\n",
    "\n",
    "    df[\"wear_max\"] = df[[\"flute_1\", \"flute_2\", \"flute_3\"]].max(axis=1)\n",
    "    EOL = float(df[\"wear_max\"].max())\n",
    "\n",
    "    eps = 1e-9\n",
    "    df[\"f1_norm\"] = df[\"flute_1\"] / (EOL + eps)\n",
    "    df[\"f2_norm\"] = df[\"flute_2\"] / (EOL + eps)\n",
    "    df[\"f3_norm\"] = df[\"flute_3\"] / (EOL + eps)\n",
    "    df[\"wear_norm\"] = df[\"wear_max\"] / (EOL + eps)\n",
    "    df[\"rul_norm\"]  = 1.0 - df[\"wear_norm\"]\n",
    "    df[\"RUL\"] = EOL - df[\"wear_max\"]\n",
    "\n",
    "    return df.sort_values(\"cut_number\").reset_index(drop=True), EOL\n",
    "\n",
    "\n",
    "def discover_cut_files(cutter_dir: str, cutter_id: int) -> Dict[int, str]:\n",
    "    all_csvs = glob.glob(os.path.join(cutter_dir, \"**\", \"*.csv\"), recursive=True)\n",
    "    all_csvs = [p for p in all_csvs if \"wear\" not in os.path.basename(p).lower()]\n",
    "    cuts = {}\n",
    "    for p in all_csvs:\n",
    "        name = os.path.basename(p).lower()\n",
    "        m = re.search(rf\"c[_-]?{cutter_id}[_-]?(\\d+)\\.csv$\", name) or re.search(r\"(\\d+)\\.csv$\", name)\n",
    "        if m:\n",
    "            cuts[int(m.group(1))] = p\n",
    "    return dict(sorted(cuts.items()))\n",
    "\n",
    "\n",
    "def build_index_for_cutters(cutters: List[str], labeled: bool = True):\n",
    "    index = []\n",
    "    eol_map = {}\n",
    "    for cname in cutters:\n",
    "        cutter_dir = os.path.join(cfg.base, cname)\n",
    "        cutter_id = int(re.findall(r\"\\d+\", cname)[0])\n",
    "        cut_files = discover_cut_files(cutter_dir, cutter_id)\n",
    "\n",
    "        if labeled:\n",
    "            wear_df, EOL = read_wear_table(cutter_dir)\n",
    "            eol_map[cname] = EOL\n",
    "            present = sorted(set(wear_df[\"cut_number\"].astype(int)).intersection(cut_files.keys()))\n",
    "            for cutn in present:\n",
    "                row = wear_df.loc[wear_df[\"cut_number\"] == cutn].iloc[0]\n",
    "                y_norm = np.array([row[\"f1_norm\"], row[\"f2_norm\"], row[\"f3_norm\"], row[\"wear_norm\"], row[\"rul_norm\"]], dtype=np.float32)\n",
    "                y_raw  = np.array([row[\"flute_1\"], row[\"flute_2\"], row[\"flute_3\"], row[\"wear_max\"], row[\"RUL\"]], dtype=np.float32)\n",
    "                index.append({\n",
    "                    \"cutter\": cname, \"eol\": EOL,\n",
    "                    \"cut_number\": int(cutn),\n",
    "                    \"path\": cut_files[int(cutn)],\n",
    "                    \"y_norm\": y_norm, \"y_raw\": y_raw\n",
    "                })\n",
    "        else:\n",
    "            present = sorted(cut_files.keys())\n",
    "            for cutn in present:\n",
    "                index.append({\n",
    "                    \"cutter\": cname, \"eol\": None,\n",
    "                    \"cut_number\": int(cutn),\n",
    "                    \"path\": cut_files[int(cutn)],\n",
    "                    \"y_norm\": None, \"y_raw\": None\n",
    "                })\n",
    "    return index, eol_map\n",
    "\n",
    "# =========================\n",
    "# features.py\n",
    "# =========================\n",
    "\"\"\"\n",
    "Feature extraction for PHM2010 MonoSeq-RUL.\n",
    "- Reads a single cut CSV (7 channels)\n",
    "- Splits into windows (WIN, HOP)\n",
    "- Computes per-window features (time + freq)\n",
    "- Aggregates to fixed-length feature sequence (MAX_WINDOWS)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import skew, kurtosis\n",
    "from config import cfg\n",
    "\n",
    "_PER_CH_FEATURES = 9  # mean,std,min,max,skew,kurt,energy,psd_mean,psd_max\n",
    "\n",
    "\n",
    "def _safe_read_csv(path: str) -> np.ndarray:\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None, engine=\"c\", low_memory=False)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None, engine=\"python\", low_memory=False)\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    for c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(axis=0, how=\"any\")\n",
    "    return df.values.astype(np.float32)\n",
    "\n",
    "\n",
    "def _feat_window(win: np.ndarray, fs: int = cfg.fs) -> np.ndarray:\n",
    "    feats = []\n",
    "    for i in range(win.shape[1]):\n",
    "        x = win[:, i]\n",
    "        feats += [x.mean(), x.std(), x.min(), x.max(), skew(x), kurtosis(x)]\n",
    "        feats.append(float(np.sum(x**2) / max(len(x), 1)))\n",
    "        f, Pxx = welch(x, fs=fs, nperseg=min(1024, len(x)))\n",
    "        feats.append(float(Pxx.mean()))\n",
    "        feats.append(float(Pxx.max()))\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "\n",
    "def extract_feature_sequence(path: str) -> np.ndarray:\n",
    "    arr = _safe_read_csv(path)\n",
    "    N, C = arr.shape\n",
    "    feats = []\n",
    "    if N >= cfg.win:\n",
    "        for s in range(0, N - cfg.win + 1, cfg.hop):\n",
    "            feats.append(_feat_window(arr[s:s+cfg.win]))\n",
    "            if len(feats) >= cfg.max_windows:\n",
    "                break\n",
    "    else:\n",
    "        # single window on the whole signal if shorter than WIN\n",
    "        feats.append(_feat_window(arr))\n",
    "\n",
    "    feats = np.stack(feats, axis=0)  # [T, 9*7=63]\n",
    "    T = feats.shape[0]\n",
    "    if T < cfg.max_windows:\n",
    "        pad = np.zeros((cfg.max_windows - T, feats.shape[1]), dtype=np.float32)\n",
    "        feats = np.vstack([feats, pad])\n",
    "    else:\n",
    "        feats = feats[:cfg.max_windows]\n",
    "    return feats.astype(np.float32)\n",
    "\n",
    "# =========================\n",
    "# dataset.py\n",
    "# =========================\n",
    "\"\"\"\n",
    "Dataset for MonoSeq-RUL.\n",
    "- Wraps cut indices into sequences of window-features\n",
    "- Provides normalized wear targets per time-step (replicated across windows)\n",
    "- Batches to [B, T, F] for the model\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from config import cfg\n",
    "from data import build_index_for_cutters\n",
    "from features import extract_feature_sequence\n",
    "\n",
    "class PHMSeqDataset(Dataset):\n",
    "    def __init__(self, index, labeled: bool = True):\n",
    "        self.index = index\n",
    "        self.labeled = labeled\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        it = self.index[i]\n",
    "        X = extract_feature_sequence(it[\"path\"])  # [T,F]\n",
    "        T, F = X.shape\n",
    "        if self.labeled and it[\"y_norm\"] is not None:\n",
    "            wear_norm = np.float32(it[\"y_norm\"][3])  # wear_norm scalar\n",
    "            y_seq = np.full((T,), wear_norm, dtype=np.float32)\n",
    "            eol = np.float32(it[\"eol\"]) if it[\"eol\"] is not None else np.float32(np.nan)\n",
    "        else:\n",
    "            y_seq = np.full((T,), np.nan, dtype=np.float32)\n",
    "            eol = np.float32(np.nan)\n",
    "        return (\n",
    "            torch.tensor(X, dtype=torch.float32),  # [T,F]\n",
    "            torch.tensor(y_seq, dtype=torch.float32),\n",
    "            torch.tensor(eol, dtype=torch.float32),\n",
    "            np.int32(it[\"cut_number\"]),\n",
    "            str(it[\"cutter\"]),\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X, y, eol, cutn, cutter = zip(*batch)\n",
    "    X = torch.stack(X)           # [B,T,F]\n",
    "    y = torch.stack(y)           # [B,T]\n",
    "    eol = torch.stack(eol)       # [B]\n",
    "    return X, y, eol, np.array(cutn, dtype=int), np.array(cutter)\n",
    "\n",
    "# =========================\n",
    "# model.py\n",
    "# =========================\n",
    "\"\"\"\n",
    "MonoSeq model: encoder + monotone increment head (cum-sum) + variance head.\n",
    "Returns wear (normalized) over windows, its log-variance, phase logits (per-step), and increments.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from config import cfg\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden: int = cfg.hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden, batch_first=True, bidirectional=True)\n",
    "        self.proj = nn.Linear(hidden*2, hidden)\n",
    "\n",
    "    def forward(self, x):  # x: [B,T,F]\n",
    "        h, _ = self.rnn(x)\n",
    "        h = F.relu(self.proj(h))  # [B,T,H]\n",
    "        return h\n",
    "\n",
    "class MonoSeqModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden: int = cfg.hidden, n_phases: int = 3):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden)\n",
    "        self.inc_raw = nn.Linear(hidden, 1)       # increments before softplus\n",
    "        self.wear_logvar = nn.Linear(hidden, 1)   # log variance for wear\n",
    "        self.phase_head = nn.Linear(hidden, n_phases)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)                   # [B,T,H]\n",
    "        inc = F.softplus(self.inc_raw(h)).squeeze(-1)      # [B,T]  (nonnegative)\n",
    "        wear = torch.cumsum(inc, dim=1)                     # [B,T]  (monotone)\n",
    "        wear_logvar = self.wear_logvar(h).squeeze(-1)       # [B,T]\n",
    "        phase_logits = self.phase_head(h)                   # [B,T,3]\n",
    "        return wear, wear_logvar, phase_logits, inc\n",
    "\n",
    "# =========================\n",
    "# losses.py\n",
    "# =========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def nll_gaussian(y_pred, y_logvar, y_true, mask=None):\n",
    "    if mask is None:\n",
    "        mask = torch.ones_like(y_true)\n",
    "    var = torch.exp(y_logvar)\n",
    "    loss = 0.5 * (torch.log(var) + (y_true - y_pred) ** 2 / (var + 1e-9))\n",
    "    loss = loss * mask\n",
    "    return loss.sum() / (mask.sum() + 1e-9)\n",
    "\n",
    "\n",
    "def monotonic_smoothness_loss(increments, mask=None, lambda_smooth=0.1):\n",
    "    if mask is None:\n",
    "        mask = torch.ones_like(increments)\n",
    "    neg_penalty = torch.relu(-increments) * mask\n",
    "    diff = increments[:, 1:] - increments[:, :-1]\n",
    "    smooth_penalty = diff.abs() * mask[:, 1:]\n",
    "    loss = neg_penalty.sum() + lambda_smooth * smooth_penalty.sum()\n",
    "    return loss / (mask.sum() + 1e-9)\n",
    "\n",
    "# (phase loss reserved; set lambda_phase=0 for now)\n",
    "\n",
    "# =========================\n",
    "# train.py\n",
    "# =========================\n",
    "\"\"\"\n",
    "End-to-end training + validation plots for MonoSeq-RUL (normalized wear).\n",
    "Outputs:\n",
    "- best model weights to models/best_monoseq.pt\n",
    "- validation CSV + plots per cutter (wear & RUL in raw units)\n",
    "- test predictions (approx, using median EOL of train) CSV + plots\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import cfg\n",
    "from data import build_index_for_cutters\n",
    "from features import extract_feature_sequence\n",
    "from dataset import PHMSeqDataset, collate_fn\n",
    "from model import MonoSeqModel\n",
    "from losses import nll_gaussian, monotonic_smoothness_loss\n",
    "\n",
    "\n",
    "def cutterwise_split(index, val_ratio):\n",
    "    # last val_ratio of cuts per cutter -> validation\n",
    "    train_idx, val_idx = [], []\n",
    "    by_c = {}\n",
    "    for i, it in enumerate(index):\n",
    "        by_c.setdefault(it[\"cutter\"], []).append(i)\n",
    "    for c, idxs in by_c.items():\n",
    "        # keep order by cut_number\n",
    "        idxs = sorted(idxs, key=lambda k: index[k][\"cut_number\"])\n",
    "        k = max(1, int(round(len(idxs)*val_ratio)))\n",
    "        train_idx += idxs[:-k]\n",
    "        val_idx   += idxs[-k:]\n",
    "    return train_idx, val_idx\n",
    "\n",
    "\n",
    "def plot_truth_pred(x, y_true, y_pred, title, ylab, path):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(x, y_true, label=\"True\", linewidth=2.2)\n",
    "    plt.plot(x, y_pred, label=\"Pred\", linestyle=\"--\", linewidth=2.2)\n",
    "    plt.title(title); plt.xlabel(\"Cut #\"); plt.ylabel(ylab); plt.grid(True, ls='--', alpha=0.5)\n",
    "    plt.legend(); plt.tight_layout(); plt.savefig(path, dpi=200); plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(cfg.seed); np.random.seed(cfg.seed)\n",
    "\n",
    "    # --- Build indices ---\n",
    "    train_index, train_eols = build_index_for_cutters(cfg.train_cutters, labeled=True)\n",
    "    test_index,  _         = build_index_for_cutters(cfg.test_cutters,  labeled=False)\n",
    "    print(f\"Train cuts: {len(train_index)} | Test cuts: {len(test_index)}\")\n",
    "\n",
    "    # --- Split train/val by cutter tail ---\n",
    "    tr_ids, va_ids = cutterwise_split(train_index, cfg.val_ratio)\n",
    "    tr_index = [train_index[i] for i in tr_ids]\n",
    "    va_index = [train_index[i] for i in va_ids]\n",
    "\n",
    "    # --- Datasets ---\n",
    "    train_ds = PHMSeqDataset(tr_index, labeled=True)\n",
    "    val_ds   = PHMSeqDataset(va_index, labeled=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=0, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    # --- Determine input dim ---\n",
    "    Fdim = extract_feature_sequence(tr_index[0][\"path\"]).shape[1]\n",
    "\n",
    "    # --- Model ---\n",
    "    model = MonoSeqModel(input_dim=Fdim).to(cfg.device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "    best = 1e18\n",
    "    for ep in range(1, cfg.epochs+1):\n",
    "        # Train\n",
    "        model.train(); tr_loss = 0.0; n_tr = 0\n",
    "        for X, y, eol, cutn, cutter in train_loader:\n",
    "            X = X.to(cfg.device); y = y.to(cfg.device)\n",
    "            opt.zero_grad()\n",
    "            wear, wear_logvar, phase_logits, inc = model(X)\n",
    "            mask = torch.isfinite(y).float()\n",
    "            loss_w = nll_gaussian(wear, wear_logvar, y, mask)\n",
    "            loss_m = monotonic_smoothness_loss(inc, mask, lambda_smooth=cfg.lambda_smooth_delta)\n",
    "            loss = loss_w + loss_m\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            opt.step()\n",
    "            tr_loss += float(loss.item()) * X.size(0); n_tr += X.size(0)\n",
    "        tr_loss /= max(1, n_tr)\n",
    "\n",
    "        # Val\n",
    "        model.eval(); va_loss = 0.0; n_va = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y, eol, cutn, cutter in val_loader:\n",
    "                X = X.to(cfg.device); y = y.to(cfg.device)\n",
    "                wear, wear_logvar, phase_logits, inc = model(X)\n",
    "                mask = torch.isfinite(y).float()\n",
    "                loss_w = nll_gaussian(wear, wear_logvar, y, mask)\n",
    "                loss_m = monotonic_smoothness_loss(inc, mask, lambda_smooth=cfg.lambda_smooth_delta)\n",
    "                loss = loss_w + loss_m\n",
    "                va_loss += float(loss.item()) * X.size(0); n_va += X.size(0)\n",
    "        va_loss /= max(1, n_va)\n",
    "        sched.step(va_loss)\n",
    "\n",
    "        print(f\"Epoch {ep:02d} | train {tr_loss:.4f} | val {va_loss:.4f}\")\n",
    "\n",
    "        if va_loss < best:\n",
    "            best = va_loss\n",
    "            torch.save(model.state_dict(), (cfg.model_dir/\"best_monoseq.pt\").as_posix())\n",
    "            print(\"  ↳ saved best\")\n",
    "\n",
    "    # --- Validation metrics/plots (raw units) ---\n",
    "    model.eval()\n",
    "    Yw=[]; Pw=[]; Yr=[]; Pr=[]; cc=[]; ct=[]\n",
    "    with torch.no_grad():\n",
    "        for X, y, eol, cutn, cutter in val_loader:\n",
    "            X = X.to(cfg.device); y = y.to(cfg.device); eol = eol.numpy()\n",
    "            wear, wear_logvar, _, _ = model(X)\n",
    "            # use last time step prediction per cut\n",
    "            wear_n = wear[:, -1].cpu().numpy()  # normalized\n",
    "            wear_raw_pred = wear_n * eol\n",
    "            rul_raw_pred  = (1.0 - wear_n) * eol\n",
    "            # ground truth (constant along T)\n",
    "            y_n = y[:, -1].cpu().numpy()\n",
    "            wear_raw_true = y_n * eol\n",
    "            rul_raw_true  = (1.0 - y_n) * eol\n",
    "            Yw.append(wear_raw_true); Pw.append(wear_raw_pred)\n",
    "            Yr.append(rul_raw_true);  Pr.append(rul_raw_pred)\n",
    "            cc.append(cutn); ct.append(cutter)\n",
    "    Yw = np.concatenate(Yw); Pw = np.concatenate(Pw)\n",
    "    Yr = np.concatenate(Yr); Pr = np.concatenate(Pr)\n",
    "\n",
    "    def rmse(a,b):\n",
    "        return float(np.sqrt(np.mean((a-b)**2)))\n",
    "    print(f\"Val Wear RMSE {rmse(Yw,Pw):.2f} | R² {np.corrcoef(Yw,Pw)[0,1]**2:.3f}\")\n",
    "    print(f\"Val  RUL RMSE {rmse(Yr,Pr):.2f} | R² {np.corrcoef(Yr,Pr)[0,1]**2:.3f}\")\n",
    "\n",
    "    # per-cutter plots on validation\n",
    "    # rebuild a simple loader to access per-item info\n",
    "    val_items = [va_index[i] for i in range(len(va_index))]\n",
    "    by_c = {}\n",
    "    for it in val_items:\n",
    "        by_c.setdefault(it[\"cutter\"], []).append(it)\n",
    "\n",
    "    for cname, items in by_c.items():\n",
    "        xs=[]; yt_w=[]; yp_w=[]; yt_r=[]; yp_r=[]\n",
    "        for it in sorted(items, key=lambda t: t[\"cut_number\"]):\n",
    "            X = extract_feature_sequence(it[\"path\"])  # [T,F]\n",
    "            X_t = torch.tensor(X, dtype=torch.float32).unsqueeze(0).to(cfg.device)\n",
    "            with torch.no_grad():\n",
    "                wear, wear_logvar, _, _ = model(X_t)\n",
    "                wn = wear[:, -1].cpu().numpy()[0]\n",
    "            eol = it[\"eol\"]\n",
    "            y_n = float(it[\"y_norm\"][3])\n",
    "            xs.append(it[\"cut_number\"]) \n",
    "            yp_w.append(wn*eol); yp_r.append((1.0-wn)*eol)\n",
    "            yt_w.append(y_n*eol); yt_r.append((1.0-y_n)*eol)\n",
    "        plot_truth_pred(xs, yt_w, yp_w, f\"{cname} — Validation Wear\", \"Wear (0.001 mm)\", (cfg.plot_dir/f\"val_{cname}_wear.png\").as_posix())\n",
    "        plot_truth_pred(xs, yt_r, yp_r, f\"{cname} — Validation RUL\",  \"RUL (wear units)\", (cfg.plot_dir/f\"val_{cname}_rul.png\").as_posix())\n",
    "\n",
    "    # --- Test predictions (approx: use median EOL) ---\n",
    "    EOL_REF = float(np.median(list(train_eols.values()))) if len(train_eols)>0 else 1.0\n",
    "    by_ct = {}\n",
    "    for it in test_index:\n",
    "        by_ct.setdefault(it[\"cutter\"], []).append(it)\n",
    "    import csv\n",
    "    with open((cfg.csv_dir/\"test_predictions.csv\").as_posix(), 'w', newline='') as f:\n",
    "        w = csv.writer(f); w.writerow([\"cutter\",\"cut_number\",\"wear_pred\",\"rul_pred\"])\n",
    "        for cname, items in by_ct.items():\n",
    "            xs=[]; yp_w=[]; yp_r=[]\n",
    "            for it in sorted(items, key=lambda t: t[\"cut_number\"]):\n",
    "                X = extract_feature_sequence(it[\"path\"])  # [T,F]\n",
    "                X_t = torch.tensor(X, dtype=torch.float32).unsqueeze(0).to(cfg.device)\n",
    "                with torch.no_grad():\n",
    "                    wear, _, _, _ = model(X_t)\n",
    "                    wn = wear[:, -1].cpu().numpy()[0]\n",
    "                xs.append(it[\"cut_number\"]) \n",
    "                yp_w.append(wn*EOL_REF); yp_r.append((1.0-wn)*EOL_REF)\n",
    "                w.writerow([cname, it[\"cut_number\"], yp_w[-1], yp_r[-1]])\n",
    "            plot_truth_pred(xs, yp_w, yp_w, f\"{cname} — Test Wear (pred)\", \"Wear (0.001 mm)\", (cfg.plot_dir/f\"test_{cname}_wear.png\").as_posix())\n",
    "            plot_truth_pred(xs, yp_r, yp_r, f\"{cname} — Test RUL (pred)\",  \"RUL (wear units)\", (cfg.plot_dir/f\"test_{cname}_rul.png\").as_posix())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4462b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937db4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete AE Signal Classification System is ready!\n",
      "To use: Initialize with your .mat file path and call run_complete_pipeline()\n",
      "Example: processor = AESignalProcessor('your_file.mat')\n",
      "         results = processor.run_complete_pipeline()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import pywt\n",
    "from scipy.stats import kurtosis, skew\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AESignalProcessor:\n",
    "    \"\"\"Complete AE Signal Processing and Classification System\"\"\"\n",
    "    \n",
    "    def __init__(self, mat_file_path):\n",
    "        self.mat_file_path = mat_file_path\n",
    "        self.data = None\n",
    "        self.processed_data = {}\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Step 1: Load MATLAB v7.3 HDF5-based AE data\"\"\"\n",
    "        print(\"Loading MAT v7.3 file using h5py...\")\n",
    "        self.data = h5py.File(self.mat_file_path, 'r')\n",
    "\n",
    "        ae_all = self.data['AE_ALL']\n",
    "        print(\"Top-level classes in AE_ALL:\", list(ae_all.keys()))\n",
    "\n",
    "        classes = ['BF', 'GF', 'TF', 'NI']\n",
    "        self.processed_data = {}\n",
    "\n",
    "        for class_name in classes:\n",
    "            if class_name in ae_all:\n",
    "                ref = ae_all[class_name][0][0]  # Get reference\n",
    "                class_data = self.data[ref]    # Dereference\n",
    "                print(f\"{class_name}: shape {class_data.shape}\")  # Should be (4, 40)\n",
    "\n",
    "            # Channel 4 (index 3), shape (40,)\n",
    "                signal_refs = class_data[3, :]\n",
    "                signal_list = []\n",
    "\n",
    "                for i in range(signal_refs.shape[0]):\n",
    "                    signal_ref = signal_refs[i]\n",
    "                    signal = np.array(self.data[signal_ref])[:, 0]  # shape (1000000,)\n",
    "                    signal_list.append(signal)\n",
    "\n",
    "                self.processed_data[class_name] = signal_list\n",
    "                print(f\"  -> Extracted {len(signal_list)} signals of shape {signal_list[0].shape}\")\n",
    "            else:\n",
    "                print(f\"Class {class_name} not found in AE_ALL.\")\n",
    "\n",
    "        return self.processed_data\n",
    "\n",
    "    \n",
    "    def burst_informed_signal_processing(self, signal, fs=1000000):\n",
    "        \"\"\"Step 2: Burst-Informed Signal Processing\"\"\"\n",
    "        print(\"Applying burst-informed signal processing...\")\n",
    "        \n",
    "        # Frame-based segmentation\n",
    "        frame_length = 1024\n",
    "        hop_length = 512\n",
    "        frames = []\n",
    "        \n",
    "        for i in range(0, len(signal) - frame_length, hop_length):\n",
    "            frame = signal[i:i + frame_length]\n",
    "            frames.append(frame)\n",
    "        \n",
    "        # Adaptive wavelet decomposition with soft thresholding\n",
    "        processed_frames = []\n",
    "        for frame in frames:\n",
    "            # Wavelet decomposition\n",
    "            coeffs = pywt.wavedec(frame, 'db4', level=4)\n",
    "            \n",
    "            # Soft thresholding\n",
    "            threshold = 0.1 * np.max(np.abs(coeffs[0]))\n",
    "            coeffs_thresh = list(coeffs)\n",
    "            for i in range(1, len(coeffs)):\n",
    "                coeffs_thresh[i] = pywt.threshold(coeffs[i], threshold, 'soft')\n",
    "            \n",
    "            # Reconstruction\n",
    "            reconstructed = pywt.waverec(coeffs_thresh, 'db4')\n",
    "            processed_frames.append(reconstructed)\n",
    "        \n",
    "        # Burst-informed frame selection (select frames with high energy)\n",
    "        energies = [np.sum(frame**2) for frame in processed_frames]\n",
    "        energy_threshold = np.percentile(energies, 75)  # Top 25% energy frames\n",
    "        \n",
    "        selected_frames = [frame for frame, energy in zip(processed_frames, energies) \n",
    "                          if energy > energy_threshold]\n",
    "        \n",
    "        if len(selected_frames) == 0:\n",
    "            selected_frames = processed_frames[:10]  # Fallback\n",
    "        \n",
    "        # Denoising and data augmentation\n",
    "        denoised_signal = np.concatenate(selected_frames)\n",
    "        \n",
    "        return denoised_signal\n",
    "    \n",
    "    def extract_features(self, signal):\n",
    "        \"\"\"Step 3: Feature Extraction\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Time Domain Features\n",
    "        features['td_mean'] = np.mean(signal)\n",
    "        features['td_std'] = np.std(signal)\n",
    "        features['td_var'] = np.var(signal)\n",
    "        features['td_rms'] = np.sqrt(np.mean(signal**2))\n",
    "        features['td_peak'] = np.max(np.abs(signal))\n",
    "        features['td_crest_factor'] = features['td_peak'] / features['td_rms']\n",
    "        features['td_kurtosis'] = kurtosis(signal)\n",
    "        features['td_skewness'] = skew(signal)\n",
    "        features['td_energy'] = np.sum(signal**2)\n",
    "        features['td_zero_crossings'] = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "        \n",
    "        # Frequency Domain Features\n",
    "        fft_signal = np.fft.fft(signal)\n",
    "        freqs = np.fft.fftfreq(len(signal))\n",
    "        magnitude = np.abs(fft_signal)\n",
    "        \n",
    "        features['fd_mean_freq'] = np.mean(freqs[:len(freqs)//2])\n",
    "        features['fd_peak_freq'] = freqs[np.argmax(magnitude[:len(freqs)//2])]\n",
    "        features['fd_spectral_centroid'] = np.sum(freqs[:len(freqs)//2] * magnitude[:len(freqs)//2]) / np.sum(magnitude[:len(freqs)//2])\n",
    "        features['fd_spectral_rolloff'] = np.percentile(magnitude[:len(freqs)//2], 85)\n",
    "        features['fd_spectral_flux'] = np.sum(np.diff(magnitude[:len(freqs)//2])**2)\n",
    "        \n",
    "        # Time-Frequency Domain Features\n",
    "        f, t, Zxx = scipy.signal.stft(signal, nperseg=256)\n",
    "        spectrogram = np.abs(Zxx)\n",
    "        \n",
    "        features['tfd_spectral_energy'] = np.sum(spectrogram**2)\n",
    "        features['tfd_spectral_entropy'] = -np.sum(spectrogram * np.log(spectrogram + 1e-10))\n",
    "        features['tfd_peak_time'] = t[np.argmax(np.sum(spectrogram, axis=0))]\n",
    "        features['tfd_peak_freq'] = f[np.argmax(np.sum(spectrogram, axis=1))]\n",
    "        \n",
    "        # Higher Order Statistics\n",
    "        features['hos_moment_3'] = np.mean(signal**3)\n",
    "        features['hos_moment_4'] = np.mean(signal**4)\n",
    "        features['hos_cumulant_3'] = np.mean((signal - np.mean(signal))**3)\n",
    "        features['hos_cumulant_4'] = np.mean((signal - np.mean(signal))**4) - 3 * np.var(signal)**2\n",
    "        \n",
    "        # Burst Features\n",
    "        burst_threshold = 3 * np.std(signal)\n",
    "        burst_indices = np.where(np.abs(signal) > burst_threshold)[0]\n",
    "        \n",
    "        if len(burst_indices) > 0:\n",
    "            features['burst_count'] = len(burst_indices)\n",
    "            features['burst_duration'] = len(burst_indices) / len(signal)\n",
    "            features['burst_energy'] = np.sum(signal[burst_indices]**2)\n",
    "            features['burst_peak'] = np.max(np.abs(signal[burst_indices]))\n",
    "        else:\n",
    "            features['burst_count'] = 0\n",
    "            features['burst_duration'] = 0\n",
    "            features['burst_energy'] = 0\n",
    "            features['burst_peak'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def attention_based_fusion(self, features_dict):\n",
    "        \"\"\"Multi-domain feature fusion with attention mechanism\"\"\"\n",
    "        # Group features by domain\n",
    "        domains = {\n",
    "            'td': [k for k in features_dict.keys() if k.startswith('td_')],\n",
    "            'fd': [k for k in features_dict.keys() if k.startswith('fd_')],\n",
    "            'tfd': [k for k in features_dict.keys() if k.startswith('tfd_')],\n",
    "            'hos': [k for k in features_dict.keys() if k.startswith('hos_')],\n",
    "            'burst': [k for k in features_dict.keys() if k.startswith('burst_')]\n",
    "        }\n",
    "        \n",
    "        # Calculate attention weights based on feature variance\n",
    "        domain_features = {}\n",
    "        attention_weights = {}\n",
    "        \n",
    "        for domain, feature_keys in domains.items():\n",
    "            domain_vals = [features_dict[k] for k in feature_keys if k in features_dict]\n",
    "            if domain_vals:\n",
    "                domain_features[domain] = np.array(domain_vals)\n",
    "                attention_weights[domain] = np.var(domain_vals) + 1e-10\n",
    "        \n",
    "        # Normalize attention weights\n",
    "        total_weight = sum(attention_weights.values())\n",
    "        for domain in attention_weights:\n",
    "            attention_weights[domain] /= total_weight\n",
    "        \n",
    "        # Apply attention weights\n",
    "        weighted_features = []\n",
    "        for domain, features_arr in domain_features.items():\n",
    "            weighted_features.extend(features_arr * attention_weights[domain])\n",
    "        \n",
    "        return np.array(weighted_features)\n",
    "    \n",
    "    def feature_engineering(self, all_features, all_labels):\n",
    "        \"\"\"Step 4: Feature Engineering with Dimensionality Reduction\"\"\"\n",
    "        print(\"Performing feature engineering...\")\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        feature_matrix = np.array(all_features)\n",
    "        label_array = np.array(all_labels)\n",
    "        \n",
    "        # Variational Autoencoder for dimensionality reduction (simplified PCA)\n",
    "        pca = PCA(n_components=min(20, feature_matrix.shape[1]))\n",
    "        compressed_features = pca.fit_transform(feature_matrix)\n",
    "        \n",
    "        # Burst-guided feature selection\n",
    "        selector = SelectKBest(f_classif, k=min(15, compressed_features.shape[1]))\n",
    "        selected_features = selector.fit_transform(compressed_features, label_array)\n",
    "        \n",
    "        # Feature scaling\n",
    "        scaled_features = self.scaler.fit_transform(selected_features)\n",
    "        \n",
    "        return scaled_features, pca, selector\n",
    "    \n",
    "    class TabNet(nn.Module):\n",
    "        \"\"\"Simplified TabNet implementation\"\"\"\n",
    "        def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "            super().__init__()\n",
    "            self.feature_transformer = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(hidden_dim, output_dim)\n",
    "            )\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return self.feature_transformer(x)\n",
    "    \n",
    "    class XGBoostPyTorch(nn.Module):\n",
    "        \"\"\"PyTorch implementation of gradient boosting\"\"\"\n",
    "        def __init__(self, input_dim, output_dim, n_estimators=100):\n",
    "            super().__init__()\n",
    "            self.trees = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(input_dim, 32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(32, output_dim)\n",
    "                ) for _ in range(n_estimators)\n",
    "            ])\n",
    "            self.n_estimators = n_estimators\n",
    "            \n",
    "        def forward(self, x):\n",
    "            outputs = []\n",
    "            for tree in self.trees:\n",
    "                outputs.append(tree(x))\n",
    "            return torch.mean(torch.stack(outputs), dim=0)\n",
    "    \n",
    "    def train_intelligent_ensemble(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Step 5: Intelligent Ensemble Classifier\"\"\"\n",
    "        print(\"Training intelligent ensemble classifier...\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.LongTensor(y_train)\n",
    "        X_test_tensor = torch.FloatTensor(X_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        input_dim = X_train.shape[1]\n",
    "        output_dim = len(np.unique(y_train))\n",
    "        \n",
    "        # Initialize models\n",
    "        tabnet = self.TabNet(input_dim, output_dim)\n",
    "        xgboost_model = self.XGBoostPyTorch(input_dim, output_dim)\n",
    "        \n",
    "        # SVM using sklearn\n",
    "        svm_model = SVC(probability=True, random_state=42)\n",
    "        svm_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Random Forest Meta Classifier\n",
    "        rf_meta = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        \n",
    "        # Train TabNet\n",
    "        tabnet_optimizer = optim.Adam(tabnet.parameters(), lr=0.001)\n",
    "        tabnet_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        tabnet.train()\n",
    "        for epoch in range(50):\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                tabnet_optimizer.zero_grad()\n",
    "                outputs = tabnet(batch_x)\n",
    "                loss = tabnet_criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                tabnet_optimizer.step()\n",
    "        \n",
    "        # Train XGBoost model\n",
    "        xgboost_optimizer = optim.Adam(xgboost_model.parameters(), lr=0.001)\n",
    "        xgboost_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        xgboost_model.train()\n",
    "        for epoch in range(50):\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                xgboost_optimizer.zero_grad()\n",
    "                outputs = xgboost_model(batch_x)\n",
    "                loss = xgboost_criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                xgboost_optimizer.step()\n",
    "        \n",
    "        # Get predictions from base classifiers\n",
    "        tabnet.eval()\n",
    "        xgboost_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tabnet_pred = torch.softmax(tabnet(X_train_tensor), dim=1).numpy()\n",
    "            xgboost_pred = torch.softmax(xgboost_model(X_train_tensor), dim=1).numpy()\n",
    "        \n",
    "        svm_pred = svm_model.predict_proba(X_train)\n",
    "        \n",
    "        # Stack predictions for meta-learning\n",
    "        meta_features = np.hstack([tabnet_pred, xgboost_pred, svm_pred])\n",
    "        rf_meta.fit(meta_features, y_train)\n",
    "        \n",
    "        # Test predictions\n",
    "        with torch.no_grad():\n",
    "            tabnet_test_pred = torch.softmax(tabnet(X_test_tensor), dim=1).numpy()\n",
    "            xgboost_test_pred = torch.softmax(xgboost_model(X_test_tensor), dim=1).numpy()\n",
    "        \n",
    "        svm_test_pred = svm_model.predict_proba(X_test)\n",
    "        meta_test_features = np.hstack([tabnet_test_pred, xgboost_test_pred, svm_test_pred])\n",
    "        \n",
    "        final_predictions = rf_meta.predict(meta_test_features)\n",
    "        \n",
    "        # Store models\n",
    "        self.models = {\n",
    "            'tabnet': tabnet,\n",
    "            'xgboost': xgboost_model,\n",
    "            'svm': svm_model,\n",
    "            'meta_classifier': rf_meta\n",
    "        }\n",
    "        \n",
    "        return final_predictions\n",
    "    \n",
    "    def evaluate_output(self, y_test, predictions):\n",
    "        \"\"\"Step 6: Output Evaluation\"\"\"\n",
    "        print(\"Evaluating results...\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        report = classification_report(y_test, predictions, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "        \n",
    "        # Store results\n",
    "        self.results = {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm,\n",
    "            'predictions': predictions,\n",
    "            'true_labels': y_test\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Classification Report:\")\n",
    "        print(classification_report(y_test, predictions))\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def explain_results(self, X_test):\n",
    "        \"\"\"Generate explainable results using LIME-like feature importance\"\"\"\n",
    "        print(\"Generating explainable results...\")\n",
    "        \n",
    "        # Feature importance from Random Forest meta-classifier\n",
    "        if 'meta_classifier' in self.models:\n",
    "            feature_importance = self.models['meta_classifier'].feature_importances_\n",
    "            \n",
    "            # Create feature importance plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(range(len(feature_importance)), feature_importance)\n",
    "            plt.title('Feature Importance for AE Signal Classification')\n",
    "            plt.xlabel('Meta-Feature Index')\n",
    "            plt.ylabel('Importance')\n",
    "            plt.show()\n",
    "            \n",
    "            return feature_importance\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def run_complete_pipeline(self, num_samples_per_class=10):\n",
    "        \"\"\"Run the complete pipeline from start to finish\"\"\"\n",
    "        print(\"=== STARTING COMPLETE AE SIGNAL CLASSIFICATION PIPELINE ===\\n\")\n",
    "        \n",
    "        # Step 1: Load data\n",
    "        self.load_data()\n",
    "        \n",
    "        # Step 2-3: Process signals and extract features\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for class_name, class_data in self.processed_data.items():\n",
    "            print(f\"\\nProcessing class: {class_name}\")\n",
    "            \n",
    "            # Sample signals from the class\n",
    "            if len(class_data) > num_samples_per_class:\n",
    "                indices = np.random.choice(len(class_data), num_samples_per_class, replace=False)\n",
    "                sampled_signals = class_data[indices]\n",
    "            else:\n",
    "                sampled_signals = class_data\n",
    "            \n",
    "            for i, signal in enumerate(sampled_signals):\n",
    "                if len(signal) > 1000:  # Ensure signal is long enough\n",
    "                    # Step 2: Burst-informed signal processing\n",
    "                    processed_signal = self.burst_informed_signal_processing(signal)\n",
    "                    \n",
    "                    # Step 3: Feature extraction\n",
    "                    features = self.extract_features(processed_signal)\n",
    "                    \n",
    "                    # Attention-based fusion\n",
    "                    fused_features = self.attention_based_fusion(features)\n",
    "                    \n",
    "                    all_features.append(fused_features)\n",
    "                    all_labels.append(class_name)\n",
    "                    \n",
    "                    if (i + 1) % 5 == 0:\n",
    "                        print(f\"  Processed {i + 1} signals...\")\n",
    "        \n",
    "        # Step 4: Feature engineering\n",
    "        feature_matrix, pca, selector = self.feature_engineering(all_features, all_labels)\n",
    "        \n",
    "        # Encode labels\n",
    "        encoded_labels = self.label_encoder.fit_transform(all_labels)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            feature_matrix, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTraining set size: {X_train.shape}\")\n",
    "        print(f\"Test set size: {X_test.shape}\")\n",
    "        \n",
    "        # Step 5: Train intelligent ensemble\n",
    "        predictions = self.train_intelligent_ensemble(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        # Step 6: Evaluate results\n",
    "        results = self.evaluate_output(y_test, predictions)\n",
    "        \n",
    "        # Generate explainable results\n",
    "        feature_importance = self.explain_results(X_test)\n",
    "        \n",
    "        print(\"\\n=== PIPELINE COMPLETED SUCCESSFULLY ===\")\n",
    "        print(f\"Final Accuracy: {results['accuracy']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Usage Example (uncomment and modify path when you have the actual .mat file)\n",
    "\"\"\"\n",
    "# Initialize the processor\n",
    "processor = AESignalProcessor('path_to_your_mat_file.mat')\n",
    "\n",
    "# Run the complete pipeline\n",
    "results = processor.run_complete_pipeline(num_samples_per_class=10)\n",
    "\n",
    "# Access results\n",
    "print(\"Final Results:\")\n",
    "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "print(\"Classes:\", processor.label_encoder.classes_)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Complete AE Signal Classification System is ready!\")\n",
    "print(\"To use: Initialize with your .mat file path and call run_complete_pipeline()\")\n",
    "print(\"Example: processor = AESignalProcessor('your_file.mat')\")\n",
    "print(\"         results = processor.run_complete_pipeline()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cecbaa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MAT v7.3 file using h5py...\n",
      "Top-level classes in AE_ALL: ['BF', 'BFI', 'GF', 'GFI', 'N', 'NI', 'TF']\n",
      "BF: shape (1000000, 40)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Accessing a group is done with bytes or str, not <class 'numpy.float64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m processor \u001b[38;5;241m=\u001b[39m AESignalProcessor(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 Paper MCT\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCutting Tool Paper\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcutting tool data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmat files data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAE_ALL.mat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m processor\u001b[38;5;241m.\u001b[39mload_data()\n",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m, in \u001b[0;36mAESignalProcessor.load_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(signal_refs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     60\u001b[0m     signal_ref \u001b[38;5;241m=\u001b[39m signal_refs[i]\n\u001b[1;32m---> 61\u001b[0m     signal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[signal_ref])[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# shape (1000000,)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     signal_list\u001b[38;5;241m.\u001b[39mappend(signal)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_data[class_name] \u001b[38;5;241m=\u001b[39m signal_list\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Muhammad Umar\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\group.py:359\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m h5o\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e(name), lapl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lapl)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n\u001b[0;32m    362\u001b[0m otype \u001b[38;5;241m=\u001b[39m h5i\u001b[38;5;241m.\u001b[39mget_type(oid)\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m otype \u001b[38;5;241m==\u001b[39m h5i\u001b[38;5;241m.\u001b[39mGROUP:\n",
      "\u001b[1;31mTypeError\u001b[0m: Accessing a group is done with bytes or str, not <class 'numpy.float64'>"
     ]
    }
   ],
   "source": [
    "processor = AESignalProcessor(r'E:\\1 Paper MCT\\Cutting Tool Paper\\Dataset\\cutting tool data\\mat files data\\AE_ALL.mat')\n",
    "processor.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f78d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = processor.run_complete_pipeline(num_samples_per_class=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ba6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
